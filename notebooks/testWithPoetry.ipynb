{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jisooryu/Projects/lease-version-reliability\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import typing \n",
    "import structlog\n",
    "from lease_version_reliability.models.train import train_model\n",
    "from lease_version_reliability.models.inference import run_inference\n",
    "\n",
    "from lease_version_reliability.data.database_io import get_logo_df, get_all_data, get_reliable_data\n",
    "\n",
    "from lease_version_reliability.data.database_io import attribute_to_label_dict\n",
    "\n",
    "from lease_version_reliability.config.settings import settings \n",
    "from lease_version_reliability.data.database import (\n",
    "    CompstakServicesMySQL,\n",
    "    get_snowflake_connection,\n",
    ")\n",
    "from lease_version_reliability.data.database import cs_mysql_instance as mysql\n",
    "from lease_version_reliability.data.database_io import read_file\n",
    "\n",
    "logger = structlog.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await run_inference(download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Submitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lease_version_reliability.config.attributes import attributes\n",
    "\n",
    "col = attributes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_reliability = [s + '_reliability' for s in col]\n",
    "col.insert(0,'submitter_person_id')\n",
    "col.insert(len(col), 'general_reliability')\n",
    "col_reliability.insert(0,'submitter_person_id')\n",
    "col_reliability.insert(len(col), 'general_reliability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = submitter_df[col_reliability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.set_axis(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt \n",
    "\n",
    "temp['date_created'] = pd.Timestamp.now()\n",
    "temp['date_created'] = temp['date_created'].dt.strftime('%Y-%m-%d %X')\n",
    "temp.columns = map(lambda x: str(x).upper(), temp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from snowflake.connector.pandas_tools import pd_writer\n",
    "\n",
    "conn = get_snowflake_connection()\n",
    "engine = create_engine(f\"snowflake://{settings.SNOWFLAKE_ACCOUNT}.{settings.SNOWFLAKE_REGION}.snowflakecomputing.com\", creator=lambda: conn)\n",
    "\n",
    "with engine.connect() as con:\n",
    "    temp.to_sql('submitter', engine, schema = 'LEASE_VERSION_RELIABILITY', index=False, if_exists='append', chunksize=10000, method=pd_writer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = attributes.copy()\n",
    "col_reliability = [s + '_prob' for s in col]\n",
    "col.insert(0,'comp_data_id_version')\n",
    "col_reliability.insert(0,'comp_data_id_version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = version_df[col_reliability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.set_axis(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['date_created'] = pd.Timestamp.now()\n",
    "temp['date_created'] = temp['date_created'].dt.strftime('%Y-%m-%d %X')\n",
    "temp.columns = map(lambda x: str(x).upper(), temp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = get_snowflake_connection()\n",
    "# engine = create_engine(f\"snowflake://{settings.SNOWFLAKE_ACCOUNT}.{settings.SNOWFLAKE_REGION}.snowflakecomputing.com\", creator=lambda: conn)\n",
    "\n",
    "# with engine.connect() as con:\n",
    "#     temp.to_sql('version', engine, schema = 'LEASE_VERSION_RELIABILITY', index=False, if_exists='append', chunksize=10000, method=pd_writer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOM Error - Read ALL_DATA in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_version_max_id(db: CompstakServicesMySQL) -> typing.Any:\n",
    "    \"\"\"\n",
    "    Retrun max id of comp_version table\n",
    "    \"\"\"\n",
    "\n",
    "    query = read_file(settings.SQL_QUERY, \"version_max_id.sql\")\n",
    "\n",
    "    return await db.fetch_val(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_all_versions(\n",
    "    db: CompstakServicesMySQL,\n",
    "    min: int,\n",
    "    max: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return version data from MySQL\n",
    "    \"\"\"\n",
    "    query = read_file(settings.SQL_QUERY, \"all_data.sql\").format(min=min, max=max)\n",
    "    data = [dict(item) for item in await db.fetch_all(query)]\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def temp_get_all_data(db:CompstakServicesMySQL) -> pd.DataFrame:\n",
    "    id = await get_version_max_id(mysql)\n",
    "    logger.info(\"Start processing lease data\")\n",
    "    all_df = pd.DataFrame()\n",
    "    for i in range(0, id, settings.BATCH_CONFIG.BATCH_SIZE):\n",
    "        logger.info(f\"Processing {i + settings.BATCH_CONFIG.BATCH_SIZE}/{id}\")\n",
    "        data = await get_all_versions(mysql, i, i + settings.BATCH_CONFIG.BATCH_SIZE)\n",
    "        all_df = pd.concat([all_df, data], ignore_index=True)\n",
    "    \n",
    "    all_df = await get_logo_df(all_df)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Connected to database mysql://admin:********@localhost:3308/compstak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:30:33 [info     ] Start processing lease data\n",
      "2023-03-01 10:30:33 [info     ] Processing 500000/4017690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/_s14cy61779_h_jz170slph00000gq/T/ipykernel_97836/1252246593.py:10: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  data = [dict(item) for item in await db.fetch_all(query)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:31:57 [info     ] Processing 1000000/4017690\n",
      "2023-03-01 10:32:45 [info     ] Processing 1500000/4017690\n",
      "2023-03-01 10:33:44 [info     ] Processing 2000000/4017690\n",
      "2023-03-01 10:35:15 [info     ] Processing 2500000/4017690\n",
      "2023-03-01 10:36:42 [info     ] Processing 3000000/4017690\n",
      "2023-03-01 10:38:19 [info     ] Processing 3500000/4017690\n",
      "2023-03-01 10:40:23 [info     ] Processing 4000000/4017690\n",
      "2023-03-01 10:42:08 [info     ] Processing 4500000/4017690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    }
   ],
   "source": [
    "await mysql.connect()\n",
    "\n",
    "temp_all_df = await temp_get_all_data(mysql)\n",
    "\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2722225, 34)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    }
   ],
   "source": [
    "await mysql.connect()\n",
    "all_df = await get_all_data(mysql)\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliable Data - Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_reliable_data(\n",
    "    db: CompstakServicesMySQL,\n",
    "    min: int,\n",
    "    max: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return reliable data (more than 3 submitted versions) from MySQL\n",
    "    \"\"\"\n",
    "    query = read_file(settings.SQL_QUERY, \"reliable_data.sql\").format(min=min, max=max)\n",
    "    data = [dict(item) for item in await db.fetch_all(query)]\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def temp_get_reliable_data() -> pd.DataFrame:\n",
    "    id = await get_version_max_id(mysql)\n",
    "    logger.info(\"Start processing lease data\")\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(0, id, settings.BATCH_CONFIG.BATCH_SIZE):\n",
    "        logger.info(f\"Processing {i + settings.BATCH_CONFIG.BATCH_SIZE}/{id}\")\n",
    "        data = await get_reliable_data(mysql, i, i + settings.BATCH_CONFIG.BATCH_SIZE)\n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "    \n",
    "    df = await get_logo_df(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Connected to database mysql://admin:********@localhost:3308/compstak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-02 14:38:06 [info     ] Processed 10000/4017690\n",
      "2023-03-02 14:38:28 [info     ] Processed 100000/4017690\n",
      "2023-03-02 14:38:57 [info     ] Processed 200000/4017690\n",
      "2023-03-02 14:39:21 [info     ] Processed 300000/4017690\n",
      "2023-03-02 14:39:40 [info     ] Processed 400000/4017690\n",
      "2023-03-02 14:39:59 [info     ] Processed 500000/4017690\n",
      "2023-03-02 14:40:18 [info     ] Processed 600000/4017690\n",
      "2023-03-02 14:40:36 [info     ] Processed 700000/4017690\n",
      "2023-03-02 14:40:52 [info     ] Processed 800000/4017690\n",
      "2023-03-02 14:41:07 [info     ] Processed 900000/4017690\n",
      "2023-03-02 14:41:25 [info     ] Processed 1000000/4017690\n",
      "2023-03-02 14:41:44 [info     ] Processed 1100000/4017690\n",
      "2023-03-02 14:42:00 [info     ] Processed 1200000/4017690\n",
      "2023-03-02 14:42:16 [info     ] Processed 1300000/4017690\n",
      "2023-03-02 14:42:36 [info     ] Processed 1400000/4017690\n",
      "2023-03-02 14:42:49 [info     ] Processed 1500000/4017690\n",
      "2023-03-02 14:43:07 [info     ] Processed 1600000/4017690\n",
      "2023-03-02 14:43:26 [info     ] Processed 1700000/4017690\n",
      "2023-03-02 14:43:45 [info     ] Processed 1800000/4017690\n",
      "2023-03-02 14:44:08 [info     ] Processed 1900000/4017690\n",
      "2023-03-02 14:44:28 [info     ] Processed 2000000/4017690\n",
      "2023-03-02 14:44:50 [info     ] Processed 2100000/4017690\n",
      "2023-03-02 14:45:12 [info     ] Processed 2200000/4017690\n",
      "2023-03-02 14:45:31 [info     ] Processed 2300000/4017690\n",
      "2023-03-02 14:45:50 [info     ] Processed 2400000/4017690\n",
      "2023-03-02 14:46:10 [info     ] Processed 2500000/4017690\n",
      "2023-03-02 14:46:29 [info     ] Processed 2600000/4017690\n",
      "2023-03-02 14:46:53 [info     ] Processed 2700000/4017690\n",
      "2023-03-02 14:47:14 [info     ] Processed 2800000/4017690\n",
      "2023-03-02 14:47:36 [info     ] Processed 2900000/4017690\n",
      "2023-03-02 14:47:59 [info     ] Processed 3000000/4017690\n",
      "2023-03-02 14:48:21 [info     ] Processed 3100000/4017690\n",
      "2023-03-02 14:48:42 [info     ] Processed 3200000/4017690\n",
      "2023-03-02 14:48:57 [info     ] Processed 3300000/4017690\n",
      "2023-03-02 14:49:16 [info     ] Processed 3400000/4017690\n",
      "2023-03-02 14:49:32 [info     ] Processed 3500000/4017690\n",
      "2023-03-02 14:49:50 [info     ] Processed 3600000/4017690\n",
      "2023-03-02 14:50:07 [info     ] Processed 3700000/4017690\n",
      "2023-03-02 14:50:27 [info     ] Processed 3800000/4017690\n",
      "2023-03-02 14:50:48 [info     ] Processed 3900000/4017690\n",
      "2023-03-02 14:51:06 [info     ] Processed 4000000/4017690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    }
   ],
   "source": [
    "from lease_version_reliability.data.database_io import get_reliable_data\n",
    "\n",
    "await mysql.connect()\n",
    "reliable_data = await get_reliable_data()\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-01 11:27:14 [info     ] Start processing lease data\n",
      "2023-03-01 11:27:14 [info     ] Processing 500000/4017690\n",
      "2023-03-01 11:28:26 [info     ] Processing 1000000/4017690\n",
      "2023-03-01 11:29:03 [info     ] Processing 1500000/4017690\n",
      "2023-03-01 11:29:45 [info     ] Processing 2000000/4017690\n",
      "2023-03-01 11:30:49 [info     ] Processing 2500000/4017690\n",
      "2023-03-01 11:31:46 [info     ] Processing 3000000/4017690\n",
      "2023-03-01 11:32:48 [info     ] Processing 3500000/4017690\n",
      "2023-03-01 11:33:39 [info     ] Processing 4000000/4017690\n",
      "2023-03-01 11:34:25 [info     ] Processing 4500000/4017690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    }
   ],
   "source": [
    "await mysql.connect()\n",
    "temp_reliable_data = await temp_get_reliable_data()\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label attributes to vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await mysql.connect()\n",
    "# reliable_data = await get_reliable_data()\n",
    "# await mysql.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = reliable_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def label_date(data, att):\n",
    "\n",
    "    idx_null = np.where((data[att + '_version'].isnull()) | (data[att + '_master'].isnull()))[0]\n",
    "    idx_execution_date =  np.where((data[att + '_version'] <= data[att + '_master']+timedelta(days=90)) & (data[att + '_version'] >= data[att + '_master']-timedelta(days=90)))[0]\n",
    "\n",
    "    data[att + '_label'] = 0 \n",
    "    data.loc[idx_null, att + '_label'] = -1\n",
    "    data.loc[idx_execution_date, att + '_label'] = 1\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lease_version_reliability.data.database_io import attribute_to_label_dict, label_tenant_name, label_strict_equality, label_lease_term\n",
    "\n",
    "def get_labels(data: pd.DataFrame, attributes: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Populate each attribute column based on label calculation rules\n",
    "    \"\"\"\n",
    "    for att in attributes:\n",
    "        logger.info(f\"Calculating Labels: {att}\")\n",
    "        data[att + \"_filled\"] = np.where(\n",
    "            (pd.notnull(data[att + \"_version\"])),\n",
    "            1,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        if ((att == 'execution_date') | (att == 'commencement_date') | (att == 'expiration_date')):\n",
    "            data = label_date(data, att)\n",
    "\n",
    "        else:\n",
    "            data[att + \"_label\"] = data.apply(\n",
    "            lambda x: attribute_to_label_dict[att](\n",
    "                x[att + \"_version\"],\n",
    "                x[att + \"_master\"],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lease_version_reliability.data.database_io import label_tenant_name\n",
    "\n",
    "def label_strict_equality(\n",
    "    data:pd.DataFrame, \n",
    "    att: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Replace attribute columns with indicator values\n",
    "    Based on strict equality for masters and versions\n",
    "    \"\"\"\n",
    "    idx_null = np.where((data[att + '_version'].isnull()) | (data[att + '_master'].isnull()))[0]\n",
    "    idx_equality =  np.where((data[att + '_version'] == data[att + '_master']))[0]\n",
    "\n",
    "    data[att + '_label'] = 0 \n",
    "    data.loc[idx_null, att + '_label'] = -1\n",
    "    data.loc[idx_equality, att + '_label'] = 1\n",
    "\n",
    "    return data \n",
    "\n",
    "\n",
    "def label_transaction_size(\n",
    "    data:pd.DataFrame,\n",
    "    att:str\n",
    "\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Replace transaction_size attribute column with indicator values\n",
    "    Given size threshold for masters and versions\n",
    "    \"\"\"\n",
    "    idx_null = np.where((data[att + '_version'].isnull()) | (data[att + '_master'].isnull()))[0]\n",
    "    idx_transaction_size =  np.where((data[att + '_version'] >= 0.95 * data[att + '_master']) & (data[att + '_version'] <= 1.05 * data[att + '_master']))[0]\n",
    "\n",
    "    data[att + '_label'] = 0 \n",
    "    data.loc[idx_null, att + '_label'] = -1\n",
    "    data.loc[idx_transaction_size, att + '_label'] = 1\n",
    "\n",
    "    return data \n",
    "\n",
    "def label_lease_term(\n",
    "    data:pd.DataFrame,\n",
    "    att: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Replace lease_term attribute column with indicator values\n",
    "    Given term threshold for masters and versions\n",
    "    \"\"\"\n",
    "    idx_null = np.where((data[att + '_version'].isnull()) | (data[att + '_master'].isnull()))[0]\n",
    "    idx_equality =  np.where((data[att + '_version'] >= 0.92 * data[att + '_master']) & (data[att + '_version'] <= 1.08 * data[att + '_master']))[0]\n",
    "\n",
    "    data[att + '_label'] = 0 \n",
    "    data.loc[idx_null, att + '_label'] = -1\n",
    "    data.loc[idx_equality, att + '_label'] = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "attribute_to_label_dict = {\n",
    "    \"tenant_name\": label_tenant_name,\n",
    "    \"space_type_id\": label_strict_equality,\n",
    "    \"transaction_size\": label_transaction_size,\n",
    "    \"starting_rent\": label_strict_equality,\n",
    "    \"execution_date\": label_date,\n",
    "    \"commencement_date\": label_date,\n",
    "    \"lease_term\": label_lease_term,\n",
    "    \"expiration_date\": label_date,\n",
    "    \"work_value\": label_strict_equality,\n",
    "    \"free_months\": label_strict_equality,\n",
    "    \"transaction_type_id\": label_strict_equality,\n",
    "    \"rent_bumps_percent_bumps\": label_strict_equality,\n",
    "    \"rent_bumps_dollar_bumps\": label_strict_equality,\n",
    "    \"lease_type_id\": label_strict_equality,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lease_version_reliability.models.inference import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Connected to database mysql://admin:********@localhost:3308/compstak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-02 16:20:24 [info     ] Reading Reliable Data from MySQL\n",
      "2023-03-02 16:20:24 [info     ] Processed 50000/4017690\n",
      "2023-03-02 16:21:33 [info     ] Processed 500000/4017690\n",
      "2023-03-02 16:22:17 [info     ] Processed 1000000/4017690\n",
      "2023-03-02 16:23:03 [info     ] Processed 1500000/4017690\n",
      "2023-03-02 16:24:02 [info     ] Processed 2000000/4017690\n",
      "2023-03-02 16:25:04 [info     ] Processed 2500000/4017690\n",
      "2023-03-02 16:26:11 [info     ] Processed 3000000/4017690\n",
      "2023-03-02 16:27:05 [info     ] Processed 3500000/4017690\n",
      "2023-03-02 16:27:54 [info     ] Processed 4000000/4017690\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1546156 entries, 0 to 1546155\n",
      "Columns: 34 entries, id to logo\n",
      "dtypes: float64(20), int64(4), object(10)\n",
      "memory usage: 954.2 MB\n",
      "None\n",
      "2023-03-02 16:28:03 [info     ] Reading All Data from MySQL\n",
      "2023-03-02 16:28:03 [info     ] Processing 50000/4017690\n",
      "2023-03-02 16:29:25 [info     ] Processing 500000/4017690\n",
      "2023-03-02 16:30:18 [info     ] Processing 1000000/4017690\n",
      "2023-03-02 16:31:22 [info     ] Processing 1500000/4017690\n",
      "2023-03-02 16:32:45 [info     ] Processing 2000000/4017690\n",
      "2023-03-02 16:34:14 [info     ] Processing 2500000/4017690\n",
      "2023-03-02 16:36:02 [info     ] Processing 3000000/4017690\n",
      "2023-03-02 16:37:57 [info     ] Processing 3500000/4017690\n",
      "2023-03-02 16:39:51 [info     ] Processing 4000000/4017690\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2722225 entries, 0 to 2722224\n",
      "Columns: 34 entries, id to logo\n",
      "dtypes: float64(20), int64(4), object(10)\n",
      "memory usage: 1.6 GB\n",
      "None\n",
      "2023-03-02 16:40:08 [info     ] Data Labels - Reliable Data\n",
      "2023-03-02 16:40:08 [info     ] Calculating Labels: tenant_name\n",
      "2023-03-02 16:40:16 [info     ] Calculating Labels: space_type_id\n",
      "2023-03-02 16:40:16 [info     ] Calculating Labels: transaction_size\n",
      "2023-03-02 16:40:16 [info     ] Calculating Labels: starting_rent\n",
      "2023-03-02 16:40:16 [info     ] Calculating Labels: execution_date\n",
      "2023-03-02 16:40:19 [info     ] Calculating Labels: commencement_date\n",
      "2023-03-02 16:40:21 [info     ] Calculating Labels: lease_term\n",
      "2023-03-02 16:40:21 [info     ] Calculating Labels: expiration_date\n",
      "2023-03-02 16:40:23 [info     ] Calculating Labels: work_value\n",
      "2023-03-02 16:40:24 [info     ] Calculating Labels: free_months\n",
      "2023-03-02 16:40:24 [info     ] Calculating Labels: transaction_type_id\n",
      "2023-03-02 16:40:24 [info     ] Calculating Labels: rent_bumps_percent_bumps\n",
      "2023-03-02 16:40:24 [info     ] Calculating Labels: rent_bumps_dollar_bumps\n",
      "2023-03-02 16:40:24 [info     ] Calculating Labels: lease_type_id\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1546156 entries, 0 to 1546155\n",
      "Columns: 62 entries, id to lease_type_id_label\n",
      "dtypes: float64(20), int64(32), object(10)\n",
      "memory usage: 1.3 GB\n",
      "None\n",
      "2023-03-02 16:40:25 [info     ] Data Labels - All Data\n",
      "2023-03-02 16:40:25 [info     ] Calculating Labels: tenant_name\n",
      "2023-03-02 16:40:40 [info     ] Calculating Labels: space_type_id\n",
      "2023-03-02 16:40:40 [info     ] Calculating Labels: transaction_size\n",
      "2023-03-02 16:40:40 [info     ] Calculating Labels: starting_rent\n",
      "2023-03-02 16:40:40 [info     ] Calculating Labels: execution_date\n",
      "2023-03-02 16:40:45 [info     ] Calculating Labels: commencement_date\n",
      "2023-03-02 16:40:49 [info     ] Calculating Labels: lease_term\n",
      "2023-03-02 16:40:49 [info     ] Calculating Labels: expiration_date\n",
      "2023-03-02 16:40:53 [info     ] Calculating Labels: work_value\n",
      "2023-03-02 16:40:53 [info     ] Calculating Labels: free_months\n",
      "2023-03-02 16:40:53 [info     ] Calculating Labels: transaction_type_id\n",
      "2023-03-02 16:40:53 [info     ] Calculating Labels: rent_bumps_percent_bumps\n",
      "2023-03-02 16:40:53 [info     ] Calculating Labels: rent_bumps_dollar_bumps\n",
      "2023-03-02 16:40:53 [info     ] Calculating Labels: lease_type_id\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2722225 entries, 0 to 2722224\n",
      "Columns: 62 entries, id to lease_type_id_label\n",
      "dtypes: float64(20), int64(32), object(10)\n",
      "memory usage: 2.3 GB\n",
      "None\n",
      "2023-03-02 16:40:56 [info     ] Feature Engineering - Reliable Data\n",
      "2023-03-02 16:40:56 [info     ] Calculating submissions feature for SUBMITTER\n",
      "2023-03-02 16:40:57 [info     ] Calculating submission features BROKERAGE LOGO\n",
      "2023-03-02 16:41:00 [info     ] Merging data with features by submitter id\n",
      "2023-03-02 16:41:04 [info     ] Merging data with features by brokerage logo\n",
      "2023-03-02 16:41:09 [info     ] Converting features into rate\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1546156 entries, 0 to 1546155\n",
      "Columns: 286 entries, id to lease_type_id_logo_fill_rate\n",
      "dtypes: float64(160), int64(116), object(10)\n",
      "memory usage: 3.9 GB\n",
      "None\n",
      "2023-03-02 16:41:11 [info     ] Feature Engineering - All Data\n",
      "2023-03-02 16:41:11 [info     ] Calculating submissions feature for SUBMITTER\n",
      "2023-03-02 16:41:13 [info     ] Calculating submission features BROKERAGE LOGO\n",
      "2023-03-02 16:41:17 [info     ] Merging data with features by submitter id\n",
      "2023-03-02 16:41:24 [info     ] Merging data with features by brokerage logo\n",
      "2023-03-02 16:41:36 [info     ] Converting features into rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2722225 entries, 0 to 2722224\n",
      "Columns: 286 entries, id to lease_type_id_logo_fill_rate\n",
      "dtypes: float64(160), int64(116), object(10)\n",
      "memory usage: 6.8 GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "await mysql.connect()\n",
    "df, df_all = await load_data()\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                          int64\n",
       "submitter_person_id                         int64\n",
       "comp_data_id_version                        int64\n",
       "comp_data_id_master                         int64\n",
       "tenant_name_version                        object\n",
       "                                           ...   \n",
       "rent_bumps_dollar_bumps_logo_fill_rate    float64\n",
       "lease_type_id_submitter_correct_rate      float64\n",
       "lease_type_id_submitter_fill_rate         float64\n",
       "lease_type_id_logo_correct_rate           float64\n",
       "lease_type_id_logo_fill_rate              float64\n",
       "Length: 286, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1.0\n",
       "1          1.0\n",
       "2          1.0\n",
       "3          1.0\n",
       "4          1.0\n",
       "          ... \n",
       "1546151    0.0\n",
       "1546152    0.0\n",
       "1546153    0.0\n",
       "1546154    0.0\n",
       "1546155    0.0\n",
       "Name: lease_type_id_submitter_correct_rate, Length: 1546156, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lease_type_id_submitter_correct_rate']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lease_version_reliability.models.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:23:55 [info     ] Connecting to MySQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Connected to database mysql://admin:********@localhost:3308/compstak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:23:55 [info     ] Get Reliable Data\n",
      "2023-03-07 12:23:55 [info     ] Processed 250000/4017690\n",
      "2023-03-07 12:30:58 [info     ] Processed 2500000/4017690\n",
      "2023-03-07 12:36:19 [info     ] Data Labels - Reliable Data\n",
      "2023-03-07 12:36:19 [info     ] Calculating Labels: tenant_name\n",
      "2023-03-07 12:36:27 [info     ] Calculating Labels: space_type_id\n",
      "2023-03-07 12:36:28 [info     ] Calculating Labels: transaction_size\n",
      "2023-03-07 12:36:28 [info     ] Calculating Labels: starting_rent\n",
      "2023-03-07 12:36:28 [info     ] Calculating Labels: execution_date\n",
      "2023-03-07 12:36:30 [info     ] Calculating Labels: commencement_date\n",
      "2023-03-07 12:36:32 [info     ] Calculating Labels: lease_term\n",
      "2023-03-07 12:36:33 [info     ] Calculating Labels: expiration_date\n",
      "2023-03-07 12:36:35 [info     ] Calculating Labels: work_value\n",
      "2023-03-07 12:36:35 [info     ] Calculating Labels: free_months\n",
      "2023-03-07 12:36:35 [info     ] Calculating Labels: transaction_type_id\n",
      "2023-03-07 12:36:35 [info     ] Calculating Labels: rent_bumps_percent_bumps\n",
      "2023-03-07 12:36:35 [info     ] Calculating Labels: rent_bumps_dollar_bumps\n",
      "2023-03-07 12:36:35 [info     ] Calculating Labels: lease_type_id\n",
      "2023-03-07 12:36:35 [info     ] Feature Engineering - Reliable Data\n",
      "2023-03-07 12:36:35 [info     ] Get Submitter Features\n",
      "2023-03-07 12:36:40 [info     ] Get Brokerage Features\n",
      "2023-03-07 12:36:42 [info     ] Combine Submitter Features\n",
      "2023-03-07 12:36:42 [info     ] Combine brokerage features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col] = data[name].map(temp_dict).fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:36:45 [info     ] Done combining brokerage features\n",
      "2023-03-07 12:36:45 [info     ] Getting Rate Features\n",
      "2023-03-07 12:36:45 [info     ] tenant_name rates\n",
      "2023-03-07 12:36:45 [info     ] space_type_id rates\n",
      "2023-03-07 12:36:45 [info     ] transaction_size rates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:94: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_label] = data[f\"{c}_{name}\"] - data[f\"{replace_label}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_total] = data[f\"{replace_total}_{name}\"] - 1\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[col_filled] = data[f\"{f}_{name}\"] - data[f\"{f}\"]\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:36:45 [info     ] starting_rent rates\n",
      "2023-03-07 12:36:45 [info     ] execution_date rates\n",
      "2023-03-07 12:36:45 [info     ] commencement_date rates\n",
      "2023-03-07 12:36:45 [info     ] lease_term rates\n",
      "2023-03-07 12:36:45 [info     ] expiration_date rates\n",
      "2023-03-07 12:36:45 [info     ] work_value rates\n",
      "2023-03-07 12:36:45 [info     ] free_months rates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:36:45 [info     ] transaction_type_id rates\n",
      "2023-03-07 12:36:45 [info     ] rent_bumps_percent_bumps rates\n",
      "2023-03-07 12:36:45 [info     ] rent_bumps_dollar_bumps rates\n",
      "2023-03-07 12:36:45 [info     ] lease_type_id rates\n",
      "2023-03-07 12:36:46 [info     ] Model Training\n",
      "2023-03-07 12:36:46 [info     ] Get reliable versions by attribute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_submitter_fill_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:179: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_correct_rate\"] = np.where(\n",
      "/Users/jisooryu/Projects/lease-version-reliability/src/lease_version_reliability/features/build_features.py:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{att}_logo_fill_rate\"] = np.where(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:41:10 [info     ] Attribute: tenant_name_label\n",
      "2023-03-07 12:41:43 [info     ] tenant_name\n",
      "2023-03-07 12:41:43 [info     ] Training Data Size: 1170869\n",
      "2023-03-07 12:41:43 [info     ] tenant_name_label - Accuracy : 0.6899507375699478\n",
      "2023-03-07 12:41:43 [info     ] tenant_name_label - F1 : 0.8067787228153842\n",
      "2023-03-07 12:41:43 [info     ] 1    1279150\n",
      "0     184437\n",
      "Name: tenant_name_label, dtype: int64\n",
      "2023-03-07 12:41:43 [info     ] ----------------------------------\n",
      "2023-03-07 12:41:43 [info     ] Attribute: space_type_id_label\n",
      "2023-03-07 12:42:00 [info     ] space_type_id\n",
      "2023-03-07 12:42:00 [info     ] Training Data Size: 806084\n",
      "2023-03-07 12:42:00 [info     ] space_type_id_label - Accuracy : 0.7416411111441927\n",
      "2023-03-07 12:42:00 [info     ] space_type_id_label - F1 : 0.8484513306534945\n",
      "2023-03-07 12:42:00 [info     ] 1    973394\n",
      "0     34212\n",
      "Name: space_type_id_label, dtype: int64\n",
      "2023-03-07 12:42:00 [info     ] ----------------------------------\n",
      "2023-03-07 12:42:00 [info     ] Attribute: transaction_size_label\n",
      "2023-03-07 12:42:27 [info     ] transaction_size\n",
      "2023-03-07 12:42:27 [info     ] Training Data Size: 1161540\n",
      "2023-03-07 12:42:27 [info     ] transaction_size_label - Accuracy : 0.3479036451607349\n",
      "2023-03-07 12:42:27 [info     ] transaction_size_label - F1 : 0.4876553397926909\n",
      "2023-03-07 12:42:27 [info     ] 1    1383092\n",
      "0      68833\n",
      "Name: transaction_size_label, dtype: int64\n",
      "2023-03-07 12:42:27 [info     ] ----------------------------------\n",
      "2023-03-07 12:42:27 [info     ] Attribute: starting_rent_label\n",
      "2023-03-07 12:42:51 [info     ] starting_rent\n",
      "2023-03-07 12:42:51 [info     ] Training Data Size: 977558\n",
      "2023-03-07 12:42:51 [info     ] starting_rent_label - Accuracy : 0.7365358648062523\n",
      "2023-03-07 12:42:51 [info     ] starting_rent_label - F1 : 0.8399844925022242\n",
      "2023-03-07 12:42:51 [info     ] 1    1049812\n",
      "0     172136\n",
      "Name: starting_rent_label, dtype: int64\n",
      "2023-03-07 12:42:51 [info     ] ----------------------------------\n",
      "2023-03-07 12:42:51 [info     ] Attribute: execution_date_label\n",
      "2023-03-07 12:43:05 [info     ] execution_date\n",
      "2023-03-07 12:43:05 [info     ] Training Data Size: 685906\n",
      "2023-03-07 12:43:05 [info     ] execution_date_label - Accuracy : 0.3783656117146906\n",
      "2023-03-07 12:43:05 [info     ] execution_date_label - F1 : 0.4972171386525291\n",
      "2023-03-07 12:43:05 [info     ] 1    783007\n",
      "0     74376\n",
      "Name: execution_date_label, dtype: int64\n",
      "2023-03-07 12:43:05 [info     ] ----------------------------------\n",
      "2023-03-07 12:43:05 [info     ] Attribute: commencement_date_label\n",
      "2023-03-07 12:43:15 [info     ] commencement_date\n",
      "2023-03-07 12:43:15 [info     ] Training Data Size: 514441\n",
      "2023-03-07 12:43:15 [info     ] commencement_date_label - Accuracy : 0.4081454929982661\n",
      "2023-03-07 12:43:15 [info     ] commencement_date_label - F1 : 0.5235862932248474\n",
      "2023-03-07 12:43:15 [info     ] 1    576345\n",
      "0     66707\n",
      "Name: commencement_date_label, dtype: int64\n",
      "2023-03-07 12:43:15 [info     ] ----------------------------------\n",
      "2023-03-07 12:43:15 [info     ] Attribute: lease_term_label\n",
      "2023-03-07 12:43:40 [info     ] lease_term\n",
      "2023-03-07 12:43:40 [info     ] Training Data Size: 1071748\n",
      "2023-03-07 12:43:40 [info     ] lease_term_label - Accuracy : 0.7144488650359412\n",
      "2023-03-07 12:43:40 [info     ] lease_term_label - F1 : 0.8266965660958594\n",
      "2023-03-07 12:43:40 [info     ] 1    1220479\n",
      "0     119207\n",
      "Name: lease_term_label, dtype: int64\n",
      "2023-03-07 12:43:40 [info     ] ----------------------------------\n",
      "2023-03-07 12:43:40 [info     ] Attribute: expiration_date_label\n",
      "2023-03-07 12:43:44 [info     ] expiration_date\n",
      "2023-03-07 12:43:44 [info     ] Training Data Size: 243940\n",
      "2023-03-07 12:43:44 [info     ] expiration_date_label - Accuracy : 0.46092545830190534\n",
      "2023-03-07 12:43:44 [info     ] expiration_date_label - F1 : 0.589931646959038\n",
      "2023-03-07 12:43:44 [info     ] 1    275289\n",
      "0     29637\n",
      "Name: expiration_date_label, dtype: int64\n",
      "2023-03-07 12:43:44 [info     ] ----------------------------------\n",
      "2023-03-07 12:43:44 [info     ] Attribute: work_value_label\n",
      "2023-03-07 12:44:01 [info     ] work_value\n",
      "2023-03-07 12:44:01 [info     ] Training Data Size: 706016\n",
      "2023-03-07 12:44:01 [info     ] work_value_label - Accuracy : 0.7113339565451403\n",
      "2023-03-07 12:44:01 [info     ] work_value_label - F1 : 0.8224066476819207\n",
      "2023-03-07 12:44:01 [info     ] 1    770079\n",
      "0    112442\n",
      "Name: work_value_label, dtype: int64\n",
      "2023-03-07 12:44:01 [info     ] ----------------------------------\n",
      "2023-03-07 12:44:01 [info     ] Attribute: free_months_label\n",
      "2023-03-07 12:44:19 [info     ] free_months\n",
      "2023-03-07 12:44:19 [info     ] Training Data Size: 772457\n",
      "2023-03-07 12:44:19 [info     ] free_months_label - Accuracy : 0.37822540972995367\n",
      "2023-03-07 12:44:19 [info     ] free_months_label - F1 : 0.486828159190372\n",
      "2023-03-07 12:44:19 [info     ] 1    860130\n",
      "0    105442\n",
      "Name: free_months_label, dtype: int64\n",
      "2023-03-07 12:44:19 [info     ] ----------------------------------\n",
      "2023-03-07 12:44:19 [info     ] Attribute: transaction_type_id_label\n",
      "2023-03-07 12:44:37 [info     ] transaction_type_id\n",
      "2023-03-07 12:44:37 [info     ] Training Data Size: 726967\n",
      "2023-03-07 12:44:37 [info     ] transaction_type_id_label - Accuracy : 0.3903555589792123\n",
      "2023-03-07 12:44:37 [info     ] transaction_type_id_label - F1 : 0.5173125849510333\n",
      "2023-03-07 12:44:37 [info     ] 1    833019\n",
      "0     75690\n",
      "Name: transaction_type_id_label, dtype: int64\n",
      "2023-03-07 12:44:37 [info     ] ----------------------------------\n",
      "2023-03-07 12:44:37 [info     ] Attribute: rent_bumps_percent_bumps_label\n",
      "2023-03-07 12:44:44 [info     ] rent_bumps_percent_bumps\n",
      "2023-03-07 12:44:44 [info     ] Training Data Size: 367679\n",
      "2023-03-07 12:44:44 [info     ] rent_bumps_percent_bumps_label - Accuracy : 0.7746192341166231\n",
      "2023-03-07 12:44:44 [info     ] rent_bumps_percent_bumps_label - F1 : 0.869414486249346\n",
      "2023-03-07 12:44:44 [info     ] 1    430301\n",
      "0     29298\n",
      "Name: rent_bumps_percent_bumps_label, dtype: int64\n",
      "2023-03-07 12:44:44 [info     ] ----------------------------------\n",
      "2023-03-07 12:44:44 [info     ] Attribute: rent_bumps_dollar_bumps_label\n",
      "2023-03-07 12:44:45 [info     ] rent_bumps_dollar_bumps\n",
      "2023-03-07 12:44:45 [info     ] Training Data Size: 111961\n",
      "2023-03-07 12:44:45 [info     ] rent_bumps_dollar_bumps_label - Accuracy : 0.8492729806009074\n",
      "2023-03-07 12:44:45 [info     ] rent_bumps_dollar_bumps_label - F1 : 0.9171689408069108\n",
      "2023-03-07 12:44:45 [info     ] 1    135282\n",
      "0      4670\n",
      "Name: rent_bumps_dollar_bumps_label, dtype: int64\n",
      "2023-03-07 12:44:45 [info     ] ----------------------------------\n",
      "2023-03-07 12:44:45 [info     ] Attribute: lease_type_id_label\n",
      "2023-03-07 12:45:02 [info     ] lease_type_id\n",
      "2023-03-07 12:45:02 [info     ] Training Data Size: 722309\n",
      "2023-03-07 12:45:02 [info     ] lease_type_id_label - Accuracy : 0.734065057758974\n",
      "2023-03-07 12:45:02 [info     ] lease_type_id_label - F1 : 0.8366254108009173\n",
      "2023-03-07 12:45:02 [info     ] 1    780835\n",
      "0    122052\n",
      "Name: lease_type_id_label, dtype: int64\n",
      "2023-03-07 12:45:02 [info     ] ----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-07 12:45:03 [info     ] Saving Models\n",
      "2023-03-07 12:45:03 [info     ] Disconnecting to MySQL\n"
     ]
    }
   ],
   "source": [
    "await train_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lease-version-reliability-temp-py3.9.10",
   "language": "python",
   "name": "lease-version-reliability-temp-py3.9.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "562e7dfc617b32736c71df661251deda0bf08d043947763a310a583b3747228d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
