{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jisooryu/Projects/lease-version-reliability\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import typing \n",
    "import structlog\n",
    "from lease_version_reliability.models.train import train_model\n",
    "from lease_version_reliability.models.inference import run_inference\n",
    "\n",
    "from lease_version_reliability.data.database_io import get_logo_df, get_all_data, get_reliable_data\n",
    "\n",
    "from lease_version_reliability.data.database_io import attribute_to_label_dict\n",
    "\n",
    "from lease_version_reliability.config.settings import settings \n",
    "from lease_version_reliability.data.database import (\n",
    "    CompstakServicesMySQL,\n",
    "    get_snowflake_connection,\n",
    ")\n",
    "from lease_version_reliability.data.database import cs_mysql_instance as mysql\n",
    "from lease_version_reliability.data.database_io import read_file\n",
    "\n",
    "logger = structlog.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await run_inference(download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Submitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lease_version_reliability.config.attributes import attributes\n",
    "\n",
    "col = attributes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_reliability = [s + '_reliability' for s in col]\n",
    "col.insert(0,'submitter_person_id')\n",
    "col.insert(len(col), 'general_reliability')\n",
    "col_reliability.insert(0,'submitter_person_id')\n",
    "col_reliability.insert(len(col), 'general_reliability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = submitter_df[col_reliability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.set_axis(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt \n",
    "\n",
    "temp['date_created'] = pd.Timestamp.now()\n",
    "temp['date_created'] = temp['date_created'].dt.strftime('%Y-%m-%d %X')\n",
    "temp.columns = map(lambda x: str(x).upper(), temp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from snowflake.connector.pandas_tools import pd_writer\n",
    "\n",
    "conn = get_snowflake_connection()\n",
    "engine = create_engine(f\"snowflake://{settings.SNOWFLAKE_ACCOUNT}.{settings.SNOWFLAKE_REGION}.snowflakecomputing.com\", creator=lambda: conn)\n",
    "\n",
    "with engine.connect() as con:\n",
    "    temp.to_sql('submitter', engine, schema = 'LEASE_VERSION_RELIABILITY', index=False, if_exists='append', chunksize=10000, method=pd_writer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = attributes.copy()\n",
    "col_reliability = [s + '_prob' for s in col]\n",
    "col.insert(0,'comp_data_id_version')\n",
    "col_reliability.insert(0,'comp_data_id_version')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = version_df[col_reliability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.set_axis(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['date_created'] = pd.Timestamp.now()\n",
    "temp['date_created'] = temp['date_created'].dt.strftime('%Y-%m-%d %X')\n",
    "temp.columns = map(lambda x: str(x).upper(), temp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = get_snowflake_connection()\n",
    "# engine = create_engine(f\"snowflake://{settings.SNOWFLAKE_ACCOUNT}.{settings.SNOWFLAKE_REGION}.snowflakecomputing.com\", creator=lambda: conn)\n",
    "\n",
    "# with engine.connect() as con:\n",
    "#     temp.to_sql('version', engine, schema = 'LEASE_VERSION_RELIABILITY', index=False, if_exists='append', chunksize=10000, method=pd_writer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOM Error - Read ALL_DATA in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_version_max_id(db: CompstakServicesMySQL) -> typing.Any:\n",
    "    \"\"\"\n",
    "    Retrun max id of comp_version table\n",
    "    \"\"\"\n",
    "\n",
    "    query = read_file(settings.SQL_QUERY, \"version_max_id.sql\")\n",
    "\n",
    "    return await db.fetch_val(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_all_versions(\n",
    "    db: CompstakServicesMySQL,\n",
    "    min: int,\n",
    "    max: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return version data from MySQL\n",
    "    \"\"\"\n",
    "    query = read_file(settings.SQL_QUERY, \"all_data.sql\").format(min=min, max=max)\n",
    "    data = [dict(item) for item in await db.fetch_all(query)]\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def temp_get_all_data(db:CompstakServicesMySQL) -> pd.DataFrame:\n",
    "    id = await get_version_max_id(mysql)\n",
    "    logger.info(\"Start processing lease data\")\n",
    "    all_df = pd.DataFrame()\n",
    "    for i in range(0, id, settings.BATCH_CONFIG.BATCH_SIZE):\n",
    "        logger.info(f\"Processing {i + settings.BATCH_CONFIG.BATCH_SIZE}/{id}\")\n",
    "        data = await get_all_versions(mysql, i, i + settings.BATCH_CONFIG.BATCH_SIZE)\n",
    "        all_df = pd.concat([all_df, data], ignore_index=True)\n",
    "    \n",
    "    all_df = await get_logo_df(all_df)\n",
    "\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Connected to database mysql://admin:********@localhost:3308/compstak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:30:33 [info     ] Start processing lease data\n",
      "2023-03-01 10:30:33 [info     ] Processing 500000/4017690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/_s14cy61779_h_jz170slph00000gq/T/ipykernel_97836/1252246593.py:10: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  data = [dict(item) for item in await db.fetch_all(query)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-01 10:31:57 [info     ] Processing 1000000/4017690\n",
      "2023-03-01 10:32:45 [info     ] Processing 1500000/4017690\n",
      "2023-03-01 10:33:44 [info     ] Processing 2000000/4017690\n",
      "2023-03-01 10:35:15 [info     ] Processing 2500000/4017690\n",
      "2023-03-01 10:36:42 [info     ] Processing 3000000/4017690\n",
      "2023-03-01 10:38:19 [info     ] Processing 3500000/4017690\n",
      "2023-03-01 10:40:23 [info     ] Processing 4000000/4017690\n",
      "2023-03-01 10:42:08 [info     ] Processing 4500000/4017690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    }
   ],
   "source": [
    "await mysql.connect()\n",
    "\n",
    "temp_all_df = await temp_get_all_data(mysql)\n",
    "\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2722225, 34)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    }
   ],
   "source": [
    "await mysql.connect()\n",
    "all_df = await get_all_data(mysql)\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliable Data - Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_reliable_data(\n",
    "    db: CompstakServicesMySQL,\n",
    "    min: int,\n",
    "    max: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return reliable data (more than 3 submitted versions) from MySQL\n",
    "    \"\"\"\n",
    "    query = read_file(settings.SQL_QUERY, \"reliable_data.sql\").format(min=min, max=max)\n",
    "    data = [dict(item) for item in await db.fetch_all(query)]\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def temp_get_reliable_data() -> pd.DataFrame:\n",
    "    id = await get_version_max_id(mysql)\n",
    "    logger.info(\"Start processing lease data\")\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(0, id, settings.BATCH_CONFIG.BATCH_SIZE):\n",
    "        logger.info(f\"Processing {i + settings.BATCH_CONFIG.BATCH_SIZE}/{id}\")\n",
    "        data = await get_reliable_data(mysql, i, i + settings.BATCH_CONFIG.BATCH_SIZE)\n",
    "        df = pd.concat([df, data], ignore_index=True)\n",
    "    \n",
    "    df = await get_logo_df(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Connected to database mysql://admin:********@localhost:3308/compstak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-02 14:38:06 [info     ] Processed 10000/4017690\n",
      "2023-03-02 14:38:28 [info     ] Processed 100000/4017690\n",
      "2023-03-02 14:38:57 [info     ] Processed 200000/4017690\n",
      "2023-03-02 14:39:21 [info     ] Processed 300000/4017690\n",
      "2023-03-02 14:39:40 [info     ] Processed 400000/4017690\n",
      "2023-03-02 14:39:59 [info     ] Processed 500000/4017690\n",
      "2023-03-02 14:40:18 [info     ] Processed 600000/4017690\n",
      "2023-03-02 14:40:36 [info     ] Processed 700000/4017690\n",
      "2023-03-02 14:40:52 [info     ] Processed 800000/4017690\n",
      "2023-03-02 14:41:07 [info     ] Processed 900000/4017690\n",
      "2023-03-02 14:41:25 [info     ] Processed 1000000/4017690\n",
      "2023-03-02 14:41:44 [info     ] Processed 1100000/4017690\n",
      "2023-03-02 14:42:00 [info     ] Processed 1200000/4017690\n",
      "2023-03-02 14:42:16 [info     ] Processed 1300000/4017690\n",
      "2023-03-02 14:42:36 [info     ] Processed 1400000/4017690\n",
      "2023-03-02 14:42:49 [info     ] Processed 1500000/4017690\n",
      "2023-03-02 14:43:07 [info     ] Processed 1600000/4017690\n",
      "2023-03-02 14:43:26 [info     ] Processed 1700000/4017690\n",
      "2023-03-02 14:43:45 [info     ] Processed 1800000/4017690\n",
      "2023-03-02 14:44:08 [info     ] Processed 1900000/4017690\n",
      "2023-03-02 14:44:28 [info     ] Processed 2000000/4017690\n",
      "2023-03-02 14:44:50 [info     ] Processed 2100000/4017690\n",
      "2023-03-02 14:45:12 [info     ] Processed 2200000/4017690\n",
      "2023-03-02 14:45:31 [info     ] Processed 2300000/4017690\n",
      "2023-03-02 14:45:50 [info     ] Processed 2400000/4017690\n",
      "2023-03-02 14:46:10 [info     ] Processed 2500000/4017690\n",
      "2023-03-02 14:46:29 [info     ] Processed 2600000/4017690\n",
      "2023-03-02 14:46:53 [info     ] Processed 2700000/4017690\n",
      "2023-03-02 14:47:14 [info     ] Processed 2800000/4017690\n",
      "2023-03-02 14:47:36 [info     ] Processed 2900000/4017690\n",
      "2023-03-02 14:47:59 [info     ] Processed 3000000/4017690\n",
      "2023-03-02 14:48:21 [info     ] Processed 3100000/4017690\n",
      "2023-03-02 14:48:42 [info     ] Processed 3200000/4017690\n",
      "2023-03-02 14:48:57 [info     ] Processed 3300000/4017690\n",
      "2023-03-02 14:49:16 [info     ] Processed 3400000/4017690\n",
      "2023-03-02 14:49:32 [info     ] Processed 3500000/4017690\n",
      "2023-03-02 14:49:50 [info     ] Processed 3600000/4017690\n",
      "2023-03-02 14:50:07 [info     ] Processed 3700000/4017690\n",
      "2023-03-02 14:50:27 [info     ] Processed 3800000/4017690\n",
      "2023-03-02 14:50:48 [info     ] Processed 3900000/4017690\n",
      "2023-03-02 14:51:06 [info     ] Processed 4000000/4017690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    }
   ],
   "source": [
    "from lease_version_reliability.data.database_io import get_reliable_data\n",
    "\n",
    "await mysql.connect()\n",
    "reliable_data = await get_reliable_data()\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-01 11:27:14 [info     ] Start processing lease data\n",
      "2023-03-01 11:27:14 [info     ] Processing 500000/4017690\n",
      "2023-03-01 11:28:26 [info     ] Processing 1000000/4017690\n",
      "2023-03-01 11:29:03 [info     ] Processing 1500000/4017690\n",
      "2023-03-01 11:29:45 [info     ] Processing 2000000/4017690\n",
      "2023-03-01 11:30:49 [info     ] Processing 2500000/4017690\n",
      "2023-03-01 11:31:46 [info     ] Processing 3000000/4017690\n",
      "2023-03-01 11:32:48 [info     ] Processing 3500000/4017690\n",
      "2023-03-01 11:33:39 [info     ] Processing 4000000/4017690\n",
      "2023-03-01 11:34:25 [info     ] Processing 4500000/4017690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:databases:Disconnected from database mysql://admin:********@localhost:3308/compstak\n"
     ]
    }
   ],
   "source": [
    "await mysql.connect()\n",
    "temp_reliable_data = await temp_get_reliable_data()\n",
    "await mysql.disconnect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label attributes to vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await mysql.connect()\n",
    "# reliable_data = await get_reliable_data()\n",
    "# await mysql.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = reliable_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def label_date(data, att):\n",
    "\n",
    "    idx_null = np.where((data[att + '_version'].isnull()) | (data[att + '_master'].isnull()))[0]\n",
    "    idx_execution_date =  np.where((data[att + '_version'] <= data[att + '_master']+timedelta(days=90)) & (data[att + '_version'] >= data[att + '_master']-timedelta(days=90)))[0]\n",
    "\n",
    "    data[att + '_label'] = 0 \n",
    "    data.loc[idx_null, att + '_label'] = -1\n",
    "    data.loc[idx_execution_date, att + '_label'] = 1\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lease_version_reliability.data.database_io import attribute_to_label_dict, label_tenant_name, label_strict_equality, label_lease_term\n",
    "\n",
    "def get_labels(data: pd.DataFrame, attributes: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Populate each attribute column based on label calculation rules\n",
    "    \"\"\"\n",
    "    for att in attributes:\n",
    "        logger.info(f\"Calculating Labels: {att}\")\n",
    "        data[att + \"_filled\"] = np.where(\n",
    "            (pd.notnull(data[att + \"_version\"])),\n",
    "            1,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        if ((att == 'execution_date') | (att == 'commencement_date') | (att == 'expiration_date')):\n",
    "            data = label_date(data, att)\n",
    "\n",
    "        else:\n",
    "            data[att + \"_label\"] = data.apply(\n",
    "            lambda x: attribute_to_label_dict[att](\n",
    "                x[att + \"_version\"],\n",
    "                x[att + \"_master\"],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lease_version_reliability.data.database_io import label_tenant_name\n",
    "\n",
    "def label_strict_equality(\n",
    "    data:pd.DataFrame, \n",
    "    att: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Replace attribute columns with indicator values\n",
    "    Based on strict equality for masters and versions\n",
    "    \"\"\"\n",
    "    idx_null = np.where((data[att + '_version'].isnull()) | (data[att + '_master'].isnull()))[0]\n",
    "    idx_equality =  np.where((data[att + '_version'] == data[att + '_master']))[0]\n",
    "\n",
    "    data[att + '_label'] = 0 \n",
    "    data.loc[idx_null, att + '_label'] = -1\n",
    "    data.loc[idx_equality, att + '_label'] = 1\n",
    "\n",
    "    return data \n",
    "\n",
    "\n",
    "def label_transaction_size(\n",
    "    data:pd.DataFrame,\n",
    "    att:str\n",
    "\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Replace transaction_size attribute column with indicator values\n",
    "    Given size threshold for masters and versions\n",
    "    \"\"\"\n",
    "    idx_null = np.where((data[att + '_version'].isnull()) | (data[att + '_master'].isnull()))[0]\n",
    "    idx_transaction_size =  np.where((data[att + '_version'] >= 0.95 * data[att + '_master']) & (data[att + '_version'] <= 1.05 * data[att + '_master']))[0]\n",
    "\n",
    "    data[att + '_label'] = 0 \n",
    "    data.loc[idx_null, att + '_label'] = -1\n",
    "    data.loc[idx_transaction_size, att + '_label'] = 1\n",
    "\n",
    "    return data \n",
    "\n",
    "def label_lease_term(\n",
    "    data:pd.DataFrame,\n",
    "    att: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Replace lease_term attribute column with indicator values\n",
    "    Given term threshold for masters and versions\n",
    "    \"\"\"\n",
    "    idx_null = np.where((data[att + '_version'].isnull()) | (data[att + '_master'].isnull()))[0]\n",
    "    idx_equality =  np.where((data[att + '_version'] >= 0.92 * data[att + '_master']) & (data[att + '_version'] <= 1.08 * data[att + '_master']))[0]\n",
    "\n",
    "    data[att + '_label'] = 0 \n",
    "    data.loc[idx_null, att + '_label'] = -1\n",
    "    data.loc[idx_equality, att + '_label'] = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "attribute_to_label_dict = {\n",
    "    \"tenant_name\": label_tenant_name,\n",
    "    \"space_type_id\": label_strict_equality,\n",
    "    \"transaction_size\": label_transaction_size,\n",
    "    \"starting_rent\": label_strict_equality,\n",
    "    \"execution_date\": label_date,\n",
    "    \"commencement_date\": label_date,\n",
    "    \"lease_term\": label_lease_term,\n",
    "    \"expiration_date\": label_date,\n",
    "    \"work_value\": label_strict_equality,\n",
    "    \"free_months\": label_strict_equality,\n",
    "    \"transaction_type_id\": label_strict_equality,\n",
    "    \"rent_bumps_percent_bumps\": label_strict_equality,\n",
    "    \"rent_bumps_dollar_bumps\": label_strict_equality,\n",
    "    \"lease_type_id\": label_strict_equality,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lease-version-reliability-temp-py3.9.10",
   "language": "python",
   "name": "lease-version-reliability-temp-py3.9.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "562e7dfc617b32736c71df661251deda0bf08d043947763a310a583b3747228d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
